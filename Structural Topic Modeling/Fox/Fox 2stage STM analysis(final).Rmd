---
title: "two stage fox"
author: "Xiaoye Zhu"
date: "2026-02-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(stm)
library(quanteda)
library(readr)
library(dplyr)
library(ggplot2)
library(purrr)
library(tidyr)
library(tidytext)
```

```{r}
# Data loading and cleaning
file_path <- "C:/Users/31745/OneDrive - Duke University/SOSC314/FOX/Fox_Comment_Details (1).csv"
stop_path <- "C:/Users/31745/OneDrive - Duke University/SOSC314/hit_stopwords.txt"

raw_data <- read_csv(file_path) # [cite: 43]
chinese_stopwords <- readLines(stop_path, encoding = "UTF-8")
```

```{r}
processed_data <- raw_data %>%
  filter(!is.na(comment_text)) %>%
  mutate(
    # 1. Remove special escape characters and carriage returns (for _x000d_)
    comment_text = gsub("_x000d_", " ", comment_text), 
    # 2. Remove any other possible line breaks
    comment_text = gsub("[\r\n]", " ", comment_text),
    
    comment_text = iconv(comment_text, to = "ASCII", sub = " "), 
    like_num = as.numeric(gsub("[^0-9.]", "", as.character(like_count)))
  ) %>%
  filter(!is.na(like_num))

# Rebuild the corpus
comment_corpus <- corpus(processed_data, text_field = "comment_text")

toks <- tokens(comment_corpus, what = "word",
               remove_punc = TRUE, remove_symbols = TRUE, remove_numbers = TRUE) %>%
  tokens_remove(stopwords("en")) %>%
  tokens_wordstem(language = "en")

print(toks[1:2])
```

```{r}
# Build the document-feature matrix (DFM)
dfm_counts <- dfm(toks)

# Filter out words that appear in less than five comments to improve the computational efficiency and quality of the model
dfm_filtered <- dfm_trim(dfm_counts, min_docfreq = 5)

# Convert DFM to the STM-specific format
# The convert function of quanteda will automatically place the metadata in the corpus (such as like_num) into out$meta
out <- convert(dfm_filtered, to = "stm")

# Force the conversion of storage.mode to prevent STM errors in some system environments
out$documents <- lapply(out$documents, function(x) {
  storage.mode(x) <- "integer"
  return(x)
})
```

```{r}
# Run Structual Topic Model, k=30
stm_pure <- stm(
  documents = out$documents,
  vocab = out$vocab,
  K = 30, 
  data = out$meta, 
  init.type = "Spectral",
  max.em.its = 75, 
  verbose = TRUE
)
```

```{r}
# Save the model and processed data as .RData file
save_path <- "C:/Users/31745/OneDrive - Duke University/SOSC314/FOX/Fox_STM_Model_Result_Final.RData"
save(stm_pure, out, prep_effect, file = save_path)
```

```{r}
# # Estimate the relationship between the proportion of topics and the number of likes
# prep_effect <- estimateEffect(
#   1:30 ~ like_num, 
#   stmobj = stm_pure, 
#   metadata = out$meta
# )
# summary(prep_effect)
```


```{r}
# Top 15 topic quality assessment and screening
# 1. Calculate coherence& exclusivity
sem_coh <- semanticCoherence(stm_pure, documents = out$documents)
excl <- exclusivity(stm_pure)

# 2. Measure Quality: Sum up after standardizing "semantic coherence" and "uniqueness"
# The higher the comprehensive score, the better the balance between readability and distinctiveness of the topic
quality_score <- as.numeric(scale(sem_coh) + scale(excl))

# 3. Extract the ids of the top 15 high-quality topics and print them
top_15_topics <- order(quality_score, decreasing = TRUE)[1:15]
print("Top 15 high-quality topics selected areï¼š")
print(top_15_topics)


# Examine the significant relationship between these 15 topics and the number of likes
summary(prep_effect, topics = top_15_topics)

# Draw the picture and highlight Top 15 (in bold red) in the picture
plot(excl, sem_coh, main="Topic Quality Diagnostic (Top 15 Highlighted)", type="n")
text(excl, sem_coh, labels=1:30, 
     col = ifelse(1:30 %in% top_15_topics, "red", "blue"),
     cex = ifelse(1:30 %in% top_15_topics, 1.2, 0.8),
     font = ifelse(1:30 %in% top_15_topics, 2, 1))

# Draw the influence curve of likes for the top 3 comprehensive scores in high-quality topics
plot(prep_effect, "like_num", method = "continuous", topics = top_15_topics[1:3],
     main = "Effect of Likes on Top 3 Quality Topics",
     xlab = "Like Count",
     printlegend = TRUE)

# List the key words of these 15 high-quality topics
labelTopics(stm_pure, topics = top_15_topics)

```


```{r}
# 1. Extract the tags of all topics
all_labels <- labelTopics(stm_pure, n = 10) # Extract the first 10 words

# 2. Specifically extract FREX words and convert them into Dataframes
selected_topics <- c(26,20,25,11,27,9,2,28,1,6,30,4,29,15,17)

frex_df <- data.frame(
  Topic_ID = selected_topics,
  FREX_Keywords = apply(all_labels$frex[selected_topics, ], 1, paste, collapse = ", ")
)

# 3. Export to a local file (saved in the current working directory)
write_csv(frex_df, "High_Quality_Topics_FREX.csv")

print(frex_df)
```

```{r}
#lemmatization,Save STM result as csv
# =========================================================
# 1. Preprocessing: Establish a unique morpheme restoration mapping table
# =========================================================
# Use original tokens 
toks_raw_for_mapping <- tokens(comment_corpus, remove_punc = TRUE, remove_numbers = TRUE) %>%
  tokens_remove(stopwords("en"))

stem_map <- data.frame(word = unlist(as.list(toks_raw_for_mapping))) %>%
  mutate(stem = char_wordstem(word, language = "en")) %>%
  group_by(stem, word) %>%
  count() %>%
  group_by(stem) %>%
  slice_max(n, n = 1, with_ties = FALSE) %>%
  ungroup() %>%
  select(stem, word)

# =========================================================
# 2. Extraction: Establish a full-volume ranking database
# =========================================================
labels_deep <- labelTopics(stm_pure, n = 1000) 

# Define an internal function to extract rankings and reduce repetitive code
get_ranks <- function(metric_matrix, name) {
  map_df(selected_topics, ~data.frame(
    topic = .x, term = metric_matrix[.x, ], 
    rank = 1:ncol(metric_matrix), metric = name
  ))
}

# Obtain the rankings of all indicators at one time
all_ranks <- bind_rows(
  get_ranks(labels_deep$frex, "frex"),
  get_ranks(labels_deep$lift, "lift"),
  get_ranks(labels_deep$score, "score")
) %>% tidyr::pivot_wider(names_from = metric, values_from = rank, names_glue = "{metric}_rank")

# =========================================================
# 3. Construct data table
# =========================================================
# Obtain the Beta weights and merge all the indicators
master_topic_table <- tidy(stm_pure, matrix = "beta") %>%
  filter(topic %in% selected_topics) %>%
  left_join(all_ranks, by = c("topic", "term")) %>%
  left_join(stem_map, by = c("term" = "stem")) %>%
  mutate(readable_word = coalesce(word, term)) %>%
  select(topic, readable_word, stem = term, prob_beta = beta, 
         frex_rank, lift_rank, score_rank) %>%
  group_by(topic) %>%
  arrange(topic, desc(prob_beta)) %>%
  mutate(word_rank_by_beta = row_number()) %>% 
  ungroup()
# =========================================================
# 4. Output: Slice as needed
# =========================================================

# 1. Perform cleaning: Check both the stem and readable_word columns simultaneously
master_topic_table_clean <- master_topic_table %>%
  # Exclude any rows containing _x000d_
  filter(!grepl("_x000d_", stem) & !grepl("_x000d_", readable_word)) %>%
  group_by(topic) %>%
  # Reorder to ensure that the Top 10/50 positions are consecutive and correct
  arrange(topic, desc(prob_beta)) %>%
  mutate(word_rank_by_beta = row_number()) %>%
  ungroup()

# 2. Export the first 50 words
write_csv(master_topic_table_clean %>% filter(word_rank_by_beta <= 50), 
          "Fox_Topic_Full_50_Clean.csv")

# 3. Export the first 10 words
write_csv(master_topic_table_clean %>% filter(word_rank_by_beta <= 10), 
          "Fox_Topic_Summary_10_Clean.csv")


```

```{r}
# =========================================================
# Extract and print the FREX with the lemmatized words
# =========================================================
# 1. Extract the original stem
all_frex_stems <- labelTopics(stm_pure, n = 10)$frex

# 2. Print the lemmatized list
cat("--- Select 15 topics of lemmatized FREX words (for manual tagging) ---\n\n")

for (i in selected_topics) {
  stems <- all_frex_stems[i, ]
  
  # Search for the most frequent original word corresponding to each stem in the stem_map
  readable_words <- map_chr(stems, function(s) {
    match <- stem_map$word[stem_map$stem == s]
    if (length(match) > 0) return(match[1]) else return(s)
  })
  
  cat(paste0("Topic ", i, ": ", paste(readable_words, collapse = ", "), "\n"))
}
```

```{r}
# 1. Label topics manually
# ---------------------------------------------------------
manual_labels <- c(
  "26" = "Topic 26: ICE agents& car accident",
  "20" = "Topic 20: Emotion& Opinion",
  "25" = "Topic 25: Protesters confronted police in city",
  "11" = "Topic 11: Law enforcement& Unlawful behaviors",
  "27" = "Topic 27: Negative Attitude towards protesters",
  "9"  = "Topic 9: Dissatisfaction with Minnesota governments",
  "2"  = "Topic 2: Controversy over media credibility",
  "28" = "Topic 28: On-site video evidence of Renee Good",
  "1"  = "Topic 1: Sensing meaninglessness of discusssion",
  "6"  = "Topic 6: Ideological attack towards leftist& democrats",
  "30" = "Topic 30: Provocative words(preparation)",
  "4"  = "Topic 4: Provocative words(fight)",
  "29" = "Topic 29: Anti-church terrorist claim of domestic protesters",
  "15" = "Topic 15: Arrestment& Inprison",
  "17" = "Topic 17: Current geopolitic situation"
)

# 2. Prepare the drawing data
# ---------------------------------------------------------
td_gamma <- tidy(stm_pure, matrix = "gamma")

heatmap_data_final <- td_gamma %>%
  filter(topic %in% selected_topics) %>%
  inner_join(mutate(out$meta, document = row_number()), by = "document") %>%
  mutate(like_group = cut(like_num, 
                          breaks = c(-Inf, 0, 5, 20, 100, 500, 1000, Inf), 
                          labels = c("0 Likes", "1-5 Likes", "6-20 Likes", "20-100 Likes",
                                     "100-500 Likes", "500-1000 Likes", "1000+ Likes"))) %>%
  group_by(like_group, topic) %>%
  summarise(avg_gamma = mean(gamma), .groups = 'drop') %>%
  mutate(topic_label = manual_labels[as.character(topic)])

# 3. Plot
# ---------------------------------------------------------
ggplot(heatmap_data_final, aes(x = like_group, y = reorder(topic_label, topic), fill = avg_gamma)) +
  geom_tile(color = "white", linewidth = 0.1) + 
  scale_fill_distiller(
    palette = "YlOrRd", 
    direction = 1, 
    name = "Avg Prop.",
    trans = "sqrt" 
  ) +
  theme_minimal() +
  labs(
    title = "High-Quality Topics (Top 15) vs. Engagement",
    subtitle = "Y-axis labels are manually curated based on restored FREX keywords.",
    x = "Engagement Level (Like Groups)",
    y = ""
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 10),
    axis.text.y = element_text(size = 9),
    panel.grid = element_blank()
  )
```

